[
  {
    "id": "intersection_001",
    "roadConfiguration": {
      "roadWidth": 14.0,
      "nEgoDirectionLanes": 2,
      "nEgoOppositeDirectionLanes": 2
    },
    "egoConfiguration": {
      "egoLaneWrtCenter": 0,
      "egoSpeedStart": 13.89,
      "egoSpeedEnd": 11.11
    },
    "path": [
      {
        "location": {
          "laneId": -2,
          "laneSection": "MIDDLE",
          "distanceToInitialEgo": -10.0,
          "distanceToEgo": -10.0
        },
        "behavior": {
          "speed": 1.5
        }
      },
      {
        "location": {
          "laneId": -1,
          "laneSection": "RIGHT",
          "distanceToInitialEgo": 5.0,
          "distanceToEgo": 5.0
        },
        "behavior": {
          "speed": 1.4
        }
      },
      {
        "location": {
          "laneId": 0,
          "laneSection": "MIDDLE",
          "distanceToInitialEgo": 20.0,
          "distanceToEgo": 20.0
        },
        "behavior": {
          "speed": 1.3
        }
      },
      {
        "location": {
          "laneId": 1,
          "laneSection": "LEFT",
          "distanceToInitialEgo": 35.0,
          "distanceToEgo": 35.0
        },
        "behavior": {
          "speed": 1.4
        }
      }
    ]
  },
  {
    "id": "intersection_002",
    "roadConfiguration": {
      "roadWidth": 10.5,
      "nEgoDirectionLanes": 1,
      "nEgoOppositeDirectionLanes": 2
    },
    "egoConfiguration": {
      "egoLaneWrtCenter": 0,
      "egoSpeedStart": 11.11,
      "egoSpeedEnd": 11.11
    },
    "path": [
      {
        "location": {
          "laneId": -1,
          "laneSection": "LEFT",
          "distanceToInitialEgo": -8.0,
          "distanceToEgo": -8.0
        },
        "behavior": {
          "speed": 1.2
        }
      },
      {
        "location": {
          "laneId": 0,
          "laneSection": "RIGHT",  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Analyzing Safety Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive safety metrics\n",
    "metrics = log.metrics\n",
    "\n",
    "print(\"=== SAFETY ANALYSIS ===\")\n",
    "print(f\"Overall Safety: {'‚úÖ SAFE' if metrics['is_safe'] else '‚ö†Ô∏è  UNSAFE'}\")\n",
    "print(f\"Collision-free: {'‚úÖ YES' if metrics['is_collision_free'] else '‚ùå NO'}\")\n",
    "print(f\"\")\n",
    "print(f\"Collision Events: {metrics['n_collisions']}\")\n",
    "print(f\"TTC Events (<3s): {metrics['n_ttc_events']}\")\n",
    "print(f\"\")\n",
    "print(f\"Minimum TTC: {metrics['min_ttc']:.2f}s\")\n",
    "print(f\"Mean TTC: {metrics['mean_ttc']:.2f}s\")\n",
    "print(f\"\")\n",
    "print(f\"Minimum Distance: {metrics['min_distance_overall']:.2f}m\")\n",
    "print(f\"Mean Min Distance: {metrics['mean_min_distance']:.2f}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = log.to_dataframe()\n",
    "\n",
    "print(f\"Simulation data shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot agent trajectories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Spatial trajectories\n",
    "for agent_id in df['agent_id'].unique():\n",
    "    agent_data = df[df['agent_id'] == agent_id]\n",
    "    axes[0,0].plot(agent_data['x'], agent_data['y'], \n",
    "                   label=agent_id, linewidth=2, marker='o', markersize=2)\n",
    "\n",
    "axes[0,0].set_xlabel('X Position (m)')\n",
    "axes[0,0].set_ylabel('Y Position (m)')\n",
    "axes[0,0].set_title('Agent Trajectories')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].axis('equal')\n",
    "\n",
    "# Plot 2: Speed profiles\n",
    "for agent_id in df['agent_id'].unique():\n",
    "    agent_data = df[df['agent_id'] == agent_id]\n",
    "    axes[0,1].plot(agent_data['time'], agent_data['v'], \n",
    "                   label=agent_id, linewidth=2)\n",
    "\n",
    "axes[0,1].set_xlabel('Time (s)')\n",
    "axes[0,1].set_ylabel('Speed (m/s)')\n",
    "axes[0,1].set_title('Speed Profiles')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distance between agents over time\n",
    "ego_data = df[df['agent_id'] == 'ego']\n",
    "ped_data = df[df['agent_id'] == 'ped_0']\n",
    "\n",
    "distances = np.sqrt((ego_data['x'].values - ped_data['x'].values)**2 + \n",
    "                   (ego_data['y'].values - ped_data['y'].values)**2)\n",
    "\n",
    "axes[1,0].plot(ego_data['time'], distances, linewidth=2, color='red')\n",
    "axes[1,0].axhline(y=1.0, color='orange', linestyle='--', alpha=0.7, label='Safety threshold')\n",
    "axes[1,0].set_xlabel('Time (s)')\n",
    "axes[1,0].set_ylabel('Distance (m)')\n",
    "axes[1,0].set_title('Inter-Agent Distance')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Collision detection\n",
    "collision_times = [state.time for state in log.states if state.collisions]\n",
    "collision_indicators = [1 if state.collisions else 0 for state in log.states]\n",
    "times = [state.time for state in log.states]\n",
    "\n",
    "axes[1,1].fill_between(times, collision_indicators, \n",
    "                      alpha=0.3, color='red', label='Collision periods')\n",
    "axes[1,1].set_xlabel('Time (s)')\n",
    "axes[1,1].set_ylabel('Collision Status')\n",
    "axes[1,1].set_title('Collision Timeline')\n",
    "axes[1,1].set_ylim(-0.1, 1.1)\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('basic_simulation_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Comparing Multiple Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and compare multiple scenarios\n",
    "scenario_ids = ['intersection_001', 'intersection_002', 'intersection_003']\n",
    "results = []\n",
    "\n",
    "for scenario_id in scenario_ids:\n",
    "    try:\n",
    "        # Load scenario\n",
    "        scene = load_scenario(\"scenarios/urban_intersection.json\", scenario_id)\n",
    "        \n",
    "        # Run simulation\n",
    "        sim = ADSimulator(scene, dt=0.05, random_seed=42)\n",
    "        log = sim.run(headless=True)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'scenario_id': scenario_id,\n",
    "            'n_agents': len(scene.agents),\n",
    "            'duration': scene.duration,\n",
    "            'simulation_time': log.duration,\n",
    "            'is_safe': log.metrics['is_safe'],\n",
    "            'n_collisions': log.metrics['n_collisions'],\n",
    "            'min_ttc': log.metrics['min_ttc'],\n",
    "            'min_distance': log.metrics['min_distance_overall']\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Completed {scenario_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed {scenario_id}: {e}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(f\"\\nScenario Comparison:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scenario comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Safety comparison\n",
    "safety_counts = comparison_df['is_safe'].value_counts()\n",
    "axes[0].pie(safety_counts.values, labels=['Unsafe', 'Safe'], autopct='%1.1f%%')\n",
    "axes[0].set_title('Safety Distribution')\n",
    "\n",
    "# TTC comparison\n",
    "valid_ttc = comparison_df[comparison_df['min_ttc'] < float('inf')]\n",
    "if not valid_ttc.empty:\n",
    "    axes[1].bar(valid_ttc['scenario_id'], valid_ttc['min_ttc'])\n",
    "    axes[1].set_xlabel('Scenario')\n",
    "    axes[1].set_ylabel('Minimum TTC (s)')\n",
    "    axes[1].set_title('Minimum Time-to-Collision')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Performance comparison\n",
    "speedup = comparison_df['duration'] / comparison_df['simulation_time']\n",
    "axes[2].bar(comparison_df['scenario_id'], speedup)\n",
    "axes[2].set_xlabel('Scenario')\n",
    "axes[2].set_ylabel('Speedup (x real-time)')\n",
    "axes[2].set_title('Simulation Performance')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Export and Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export simulation data for external analysis\n",
    "df.to_csv('simulation_data.csv', index=False)\n",
    "comparison_df.to_csv('scenario_comparison.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Exported data files:\")\n",
    "print(\"  - simulation_data.csv: Detailed timestep data\")\n",
    "print(\"  - scenario_comparison.csv: Summary metrics\")\n",
    "print(\"  - basic_simulation_analysis.png: Visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n",
    "print(\"=== SIMULATION SUMMARY ===\")\n",
    "print(f\"Scenarios analyzed: {len(results)}\")\n",
    "print(f\"Safe scenarios: {comparison_df['is_safe'].sum()}/{len(comparison_df)}\")\n",
    "print(f\"Average simulation speedup: {speedup.mean():.1f}x real-time\")\n",
    "print(f\"\")\n",
    "\n",
    "if not valid_ttc.empty:\n",
    "    print(f\"TTC Statistics:\")\n",
    "    print(f\"  Mean: {valid_ttc['min_ttc'].mean():.2f}s\")\n",
    "    print(f\"  Std:  {valid_ttc['min_ttc'].std():.2f}s\")\n",
    "    print(f\"  Min:  {valid_ttc['min_ttc'].min():.2f}s\")\n",
    "    print(f\"  Max:  {valid_ttc['min_ttc'].max():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Annotation-Native**: AD-SimLite directly ingests dataset annotations without reconstruction\n",
    "2. **Fast & Deterministic**: Achieves real-time+ performance with reproducible results\n",
    "3. **Rich Metrics**: Comprehensive safety evaluation with TTC, collision detection, and distance tracking\n",
    "4. **Research-Ready**: Easy export to standard formats for further analysis\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- üéØ [**Perturbation Analysis**](02_perturbation_analysis.ipynb): Systematic scenario modifications\n",
    "- üìä [**Advanced Safety Metrics**](03_safety_metrics.ipynb): Deep dive into safety evaluation\n",
    "- üîÑ [**Monte Carlo Studies**](04_monte_carlo.ipynb): Large-scale statistical analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
